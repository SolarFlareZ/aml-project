{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparseSGDM optimizer: SGD with momentum + support for per-parameter gradient masks.\n",
    "# Usage:\n",
    "#   optimizer = SparseSGDM(model.parameters(), lr=0.1, momentum=0.9, gradient_masks=masks)\n",
    "# where `masks` is a list of tensors, one per parameter (same ordering as model.parameters()).\n",
    "# You can also set masks later with optimizer.set_param_masks(list_of_masks).\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class SparseSGDM(Optimizer):\n",
    "    \"\"\"SGD with momentum (SGDM) that supports per-parameter gradient masks.\n",
    "    Masked entries (mask == 0) receive no updates (no weight-decay, no grad, and momentum is zeroed).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, gradient_masks=None):\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov requires a momentum and zero dampening\")\n",
    "\n",
    "        # If gradient_masks provided, it must align with the flattened param list\n",
    "        if gradient_masks is not None:\n",
    "            params_list = []\n",
    "            for group in self.param_groups:\n",
    "                params_list.extend(group['params'])\n",
    "            if len(gradient_masks) != len(params_list):\n",
    "                raise ValueError(\"gradient_masks length must match number of parameters\")\n",
    "            for p, m in zip(params_list, gradient_masks):\n",
    "                # store mask in state; ensure proper device/dtype when used\n",
    "                self.state[p]['mask'] = m\n",
    "\n",
    "    def set_param_masks(self, masks):\n",
    "        \"\"\"Replace masks after init. `masks` must be list aligned with optimizer params.\"\"\"\n",
    "        params_list = []\n",
    "        for group in self.param_groups:\n",
    "            params_list.extend(group['params'])\n",
    "        if len(masks) != len(params_list):\n",
    "            raise ValueError(\"masks length must match number of parameters\")\n",
    "        for p, m in zip(params_list, masks):\n",
    "            self.state[p]['mask'] = m\n",
    "\n",
    "    def _get_mask(self, p):\n",
    "        st = self.state.get(p, {})\n",
    "        mask = st.get('mask', None)\n",
    "        if mask is None:\n",
    "            return None\n",
    "        # ensure mask is same device and dtype as parameter data\n",
    "        return mask.to(p.device).type_as(p.data)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        This is an adapted implementation of torch.optim.SGD.step with masking.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                mask = self._get_mask(p)  # may be None\n",
    "\n",
    "                # Apply mask to gradient (if provided)\n",
    "                if mask is not None:\n",
    "                    d_p = d_p.mul(mask)\n",
    "\n",
    "                # Weight decay contribution must also be masked (so masked weights are fully frozen)\n",
    "                if weight_decay != 0:\n",
    "                    wd = p.data.mul(weight_decay)\n",
    "                    if mask is not None:\n",
    "                        wd = wd.mul(mask)\n",
    "                    d_p = d_p.add(wd)\n",
    "\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state.setdefault(p, {})\n",
    "                    buf = param_state.get('momentum_buffer', None)\n",
    "                    if buf is None:\n",
    "                        # initialize buffer with current (masked) d_p\n",
    "                        buf = param_state['momentum_buffer'] = d_p.clone().detach()\n",
    "                    else:\n",
    "                        # ensure buffer is on the right device/dtype\n",
    "                        if buf.device != d_p.device:\n",
    "                            buf = buf.to(d_p.device)\n",
    "                            param_state['momentum_buffer'] = buf\n",
    "                        # multiply existing buffer by momentum then add current grad contribution\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "                    # If mask is provided, keep momentum zeroed in masked locations\n",
    "                    if mask is not None:\n",
    "                        buf.mul_(mask)\n",
    "\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                # finally apply update\n",
    "                p.data.add_(d_p, alpha=-lr)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
